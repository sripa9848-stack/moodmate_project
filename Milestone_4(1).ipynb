{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FA8CTjC_2PNC",
        "outputId": "039eed68-d799-452c-b800-0f67dab9664d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Data Extraction & Preprocessing\n",
            "Destination created: /content/extracted_fer_data\n",
            "Attempt to extract fer-2013.zip.zip...\n",
            "Extracting successful!!!\n",
            "Found 28709 images belonging to 7 classes.\n",
            "Found 3589 images belonging to 7 classes.\n",
            "Preprocessing Process Completed Successfully!!!\n",
            "Finally Milestone-1 Completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Model Summary ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m9,248\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m36,928\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │       \u001b[38;5;34m147,584\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4608\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │     \u001b[38;5;34m1,179,904\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │         \u001b[38;5;34m1,799\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4608</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,179,904</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,799</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,469,927\u001b[0m (5.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,469,927</span> (5.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,469,031\u001b[0m (5.60 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,469,031</span> (5.60 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m568s\u001b[0m 1s/step - accuracy: 0.2328 - loss: 2.1995 - val_accuracy: 0.2467 - val_loss: 1.7889 - learning_rate: 0.0010\n",
            "Epoch 2/30\n",
            "\u001b[1m  1/448\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8:44\u001b[0m 1s/step - accuracy: 0.2656 - loss: 1.9660"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:116: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 44ms/step - accuracy: 0.2656 - loss: 1.9660 - val_accuracy: 0.2469 - val_loss: 1.7920 - learning_rate: 0.0010\n",
            "Epoch 3/30\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m597s\u001b[0m 1s/step - accuracy: 0.2796 - loss: 1.7711 - val_accuracy: 0.3259 - val_loss: 1.7401 - learning_rate: 0.0010\n",
            "Epoch 4/30\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 44ms/step - accuracy: 0.3125 - loss: 1.8228 - val_accuracy: 0.3326 - val_loss: 1.7135 - learning_rate: 0.0010\n",
            "Epoch 5/30\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m584s\u001b[0m 1s/step - accuracy: 0.3017 - loss: 1.6943 - val_accuracy: 0.3923 - val_loss: 1.5168 - learning_rate: 0.0010\n",
            "Epoch 6/30\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 32ms/step - accuracy: 0.2656 - loss: 1.6072 - val_accuracy: 0.3825 - val_loss: 1.5518 - learning_rate: 0.0010\n",
            "Epoch 7/30\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m578s\u001b[0m 1s/step - accuracy: 0.3376 - loss: 1.6171 - val_accuracy: 0.4375 - val_loss: 1.4837 - learning_rate: 0.0010\n",
            "Epoch 8/30\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 43ms/step - accuracy: 0.3750 - loss: 1.4806 - val_accuracy: 0.4333 - val_loss: 1.4745 - learning_rate: 0.0010\n",
            "Epoch 9/30\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m622s\u001b[0m 1s/step - accuracy: 0.3729 - loss: 1.5518 - val_accuracy: 0.4322 - val_loss: 1.4432 - learning_rate: 0.0010\n",
            "Epoch 10/30\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 32ms/step - accuracy: 0.3750 - loss: 1.6832 - val_accuracy: 0.4255 - val_loss: 1.4510 - learning_rate: 0.0010\n",
            "Epoch 11/30\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m598s\u001b[0m 1s/step - accuracy: 0.3884 - loss: 1.5255 - val_accuracy: 0.4169 - val_loss: 1.4577 - learning_rate: 0.0010\n",
            "Epoch 12/30\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 44ms/step - accuracy: 0.4062 - loss: 1.6725 - val_accuracy: 0.4210 - val_loss: 1.4460 - learning_rate: 0.0010\n",
            "Epoch 13/30\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m534s\u001b[0m 1s/step - accuracy: 0.4074 - loss: 1.4887 - val_accuracy: 0.4880 - val_loss: 1.3349 - learning_rate: 0.0010\n",
            "Epoch 14/30\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.2812 - loss: 1.7619 - val_accuracy: 0.4914 - val_loss: 1.3239 - learning_rate: 0.0010\n",
            "Epoch 15/30\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m532s\u001b[0m 1s/step - accuracy: 0.4184 - loss: 1.4638 - val_accuracy: 0.5128 - val_loss: 1.2850 - learning_rate: 0.0010\n",
            "Epoch 16/30\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 41ms/step - accuracy: 0.4844 - loss: 1.3762 - val_accuracy: 0.5073 - val_loss: 1.2934 - learning_rate: 0.0010\n",
            "Epoch 17/30\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 1s/step - accuracy: 0.4338 - loss: 1.4242 - val_accuracy: 0.5086 - val_loss: 1.2764 - learning_rate: 0.0010\n",
            "Epoch 18/30\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 44ms/step - accuracy: 0.4688 - loss: 1.5397 - val_accuracy: 0.5039 - val_loss: 1.2883 - learning_rate: 0.0010\n",
            "Epoch 19/30\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m558s\u001b[0m 1s/step - accuracy: 0.4460 - loss: 1.4108 - val_accuracy: 0.5159 - val_loss: 1.3107 - learning_rate: 0.0010\n",
            "Epoch 20/30\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 42ms/step - accuracy: 0.4375 - loss: 1.3343 - val_accuracy: 0.5112 - val_loss: 1.3296 - learning_rate: 0.0010\n",
            "Epoch 21/30\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m542s\u001b[0m 1s/step - accuracy: 0.4732 - loss: 1.3590 - val_accuracy: 0.5385 - val_loss: 1.2300 - learning_rate: 0.0010\n",
            "Epoch 22/30\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.5156 - loss: 1.1812 - val_accuracy: 0.5332 - val_loss: 1.2337 - learning_rate: 0.0010\n",
            "Epoch 23/30\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m540s\u001b[0m 1s/step - accuracy: 0.4920 - loss: 1.3353 - val_accuracy: 0.5410 - val_loss: 1.2285 - learning_rate: 0.0010\n",
            "Epoch 24/30\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 32ms/step - accuracy: 0.5938 - loss: 1.1586 - val_accuracy: 0.5371 - val_loss: 1.2282 - learning_rate: 0.0010\n",
            "Epoch 25/30\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m559s\u001b[0m 1s/step - accuracy: 0.4912 - loss: 1.3123 - val_accuracy: 0.5650 - val_loss: 1.1592 - learning_rate: 0.0010\n",
            "Epoch 26/30\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 31ms/step - accuracy: 0.4375 - loss: 1.3304 - val_accuracy: 0.5583 - val_loss: 1.1657 - learning_rate: 0.0010\n",
            "Epoch 27/30\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m568s\u001b[0m 1s/step - accuracy: 0.5039 - loss: 1.2990 - val_accuracy: 0.5511 - val_loss: 1.1713 - learning_rate: 0.0010\n",
            "Epoch 28/30\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 45ms/step - accuracy: 0.5312 - loss: 1.2429 - val_accuracy: 0.5508 - val_loss: 1.1774 - learning_rate: 0.0010\n",
            "Epoch 29/30\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m561s\u001b[0m 1s/step - accuracy: 0.5065 - loss: 1.2891 - val_accuracy: 0.5795 - val_loss: 1.1285 - learning_rate: 0.0010\n",
            "Epoch 30/30\n",
            "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 31ms/step - accuracy: 0.4219 - loss: 1.5457 - val_accuracy: 0.5795 - val_loss: 1.1306 - learning_rate: 0.0010\n",
            "\n",
            " Milestone 2 Output Saved: Model saved to emotion_detection_cnn_model.keras\n",
            " Milestone 2 Completed!!!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "FER_ZIP_PATH = 'fer-2013.zip.zip'\n",
        "UNZIP_DESTINATION = '/content/extracted_fer_data'\n",
        "IMG_SIZE = (48, 48)\n",
        "NUM_CLASSES = 7\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 30\n",
        "print(\"Starting Data Extraction & Preprocessing\")\n",
        "\n",
        "if os.path.exists(UNZIP_DESTINATION):\n",
        "    shutil.rmtree(UNZIP_DESTINATION)\n",
        "os.makedirs(UNZIP_DESTINATION, exist_ok=True)\n",
        "print(f\"Destination created: {UNZIP_DESTINATION}\")\n",
        "\n",
        "try:\n",
        "    print(f\"Attempt to extract {FER_ZIP_PATH}...\")\n",
        "    with zipfile.ZipFile(FER_ZIP_PATH, 'r') as zip_ref:\n",
        "        zip_ref.extractall(UNZIP_DESTINATION)\n",
        "    print(\"Extracting successful!!!\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"File Not Found\")\n",
        "    exit()\n",
        "extracted_contents = os.listdir(UNZIP_DESTINATION)\n",
        "if len(extracted_contents) == 1 and os.path.isdir(os.path.join(UNZIP_DESTINATION, extracted_contents[0])):\n",
        "    FER_BASE_DIR = os.path.join(UNZIP_DESTINATION, extracted_contents[0])\n",
        "else:\n",
        "    FER_BASE_DIR = UNZIP_DESTINATION\n",
        "\n",
        "TRAIN_DIR = os.path.join(FER_BASE_DIR, 'train')\n",
        "TEST_DIR = os.path.join(FER_BASE_DIR, 'test')\n",
        "VAL_DIR = os.path.join(FER_BASE_DIR, 'validation')\n",
        "\n",
        "if not os.path.exists(TRAIN_DIR):\n",
        "    print(f\"Training Directory Not Found\")\n",
        "    exit()\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=10, width_shift_range=0.1, height_shift_range=0.1,\n",
        "    shear_range=0.1, zoom_range=0.1, horizontal_flip=True, fill_mode='nearest'\n",
        ")\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    TRAIN_DIR, target_size=IMG_SIZE, batch_size=BATCH_SIZE, color_mode='grayscale', class_mode='categorical'\n",
        ")\n",
        "validation_dir = VAL_DIR if os.path.exists(VAL_DIR) else TEST_DIR\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    validation_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, color_mode='grayscale', class_mode='categorical'\n",
        ")\n",
        "print(\"Preprocessing Process Completed Successfully!!!\")\n",
        "print(\"Finally Milestone-1 Completed\")\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(IMG_SIZE[0], IMG_SIZE[1], 1)),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.25),\n",
        "    Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.25),\n",
        "    Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.25),\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(NUM_CLASSES, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001)\n",
        "]\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"\\n--- Model Summary ---\")\n",
        "model.summary()\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // BATCH_SIZE,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "MODEL_SAVE_PATH = 'emotion_detection_cnn_model.keras'\n",
        "model.save(MODEL_SAVE_PATH)\n",
        "print(f\"\\n Milestone 2 Output Saved: Model saved to {MODEL_SAVE_PATH}\")\n",
        "loaded_model = tf.keras.models.load_model('emotion_detection_cnn_model.keras')\n",
        "print(\" Milestone 2 Completed!!!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "MUSIC_ZIP_PATH = 'spotify_dataset.zip'\n",
        "MUSIC_CSV_IN_ZIP = 'spotify_tracks.csv'\n",
        "OUTPUT_DIR = 'preprocessed_data'\n",
        "MODEL_SAVE_PATH = 'emotion_detection_cnn_model.keras'\n",
        "MUSIC_DATA_PATH = os.path.join(OUTPUT_DIR, 'preprocessed_music_data.csv')\n",
        "EMOTION_TO_MOOD_MAP = {\n",
        "    'happy': ['pop', 'dance', 'happy', 'party', 'upbeat', 'electronic'],\n",
        "    'sad': ['acoustic', 'blues', 'sad', 'lo-fi', 'gospel'],\n",
        "    'neutral': ['ambient', 'chill', 'study', 'sleep', 'jazz'],\n",
        "    'angry': ['rock', 'metal', 'hardcore', 'grindcore', 'punk'],\n",
        "    'fear': ['ambient', 'soundtracks', 'movies', 'drone'],\n",
        "    'surprise': ['dance', 'j-dance', 'club', 'techno'],\n",
        "    'disgust': ['industrial', 'metal-misc', 'goth']\n",
        "}\n",
        "EMOTION_LABELS = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
        "def preprocess_music_data():\n",
        "    if not os.path.exists(OUTPUT_DIR):\n",
        "        os.makedirs(OUTPUT_DIR)\n",
        "\n",
        "    print(\"Starting Music Data Preprocessing from Milestone1\")\n",
        "    try:\n",
        "        with zipfile.ZipFile(MUSIC_ZIP_PATH, 'r') as z:\n",
        "            with z.open(MUSIC_CSV_IN_ZIP) as f:\n",
        "                df = pd.read_csv(f)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Music ZIP file not found at: {MUSIC_ZIP_PATH}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR reading music CSV: {e}\")\n",
        "        return False\n",
        "\n",
        "    df = df[['name', 'artists', 'genre', 'popularity']].dropna().drop_duplicates()\n",
        "\n",
        "    def get_mood_tags(genre):\n",
        "        genre_lower = str(genre).lower()\n",
        "        all_tags = [tag for tags in EMOTION_TO_MOOD_MAP.values() for tag in tags]\n",
        "        return [tag for tag in all_tags if tag in genre_lower]\n",
        "\n",
        "    df['Mood_Tags'] = df['genre'].apply(get_mood_tags)\n",
        "    df.to_csv(MUSIC_DATA_PATH, index=False)\n",
        "    print(f\"Music Data Preprocessing Completed successfully!!! File saved to: {MUSIC_DATA_PATH}\")\n",
        "    return True\n",
        "def recommend_music(emotion_label, music_df, top_n=10):\n",
        "    if emotion_label not in EMOTION_TO_MOOD_MAP:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    required_tags = EMOTION_TO_MOOD_MAP[emotion_label.lower()]\n",
        "\n",
        "    print(f\"\\nRecommendation for: {emotion_label.upper()}\")\n",
        "    print(f\"Searching for tracks matching tags: {required_tags}\")\n",
        "\n",
        "    def check_for_tags(tags_str):\n",
        "        if pd.isna(tags_str) or tags_str == '[]' or not tags_str: return False\n",
        "        tags_str_lower = str(tags_str).lower()\n",
        "        return any(tag in tags_str_lower for tag in required_tags)\n",
        "\n",
        "    filtered_df = music_df[music_df['Mood_Tags'].apply(check_for_tags)].copy()\n",
        "\n",
        "    if filtered_df.empty:\n",
        "        print(\"No exact matching tracks found\")\n",
        "        return music_df.sort_values(by='popularity', ascending=False).head(top_n)[['name', 'artists', 'genre', 'popularity']]\n",
        "\n",
        "    recommendations = filtered_df.sort_values(by='popularity', ascending=False).head(top_n)\n",
        "\n",
        "    return recommendations[['name', 'artists', 'genre', 'popularity']]\n",
        "def run_moodmate_demo(simulated_emotion='happy'):\n",
        "    print(f\"\\nEmotion Detected from Milestone 2 Output: {simulated_emotion.capitalize()}\")\n",
        "\n",
        "    try:\n",
        "        music_df = pd.read_csv(MUSIC_DATA_PATH)\n",
        "        if os.path.exists(MODEL_SAVE_PATH):\n",
        "            print(\"Model and Music Data files verified\")\n",
        "        else:\n",
        "            print(f\"File {MODEL_SAVE_PATH} not found\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load music data Details: {e}\")\n",
        "        return\n",
        "\n",
        "    detected_emotion = simulated_emotion\n",
        "    recommendations = recommend_music(detected_emotion, music_df)\n",
        "\n",
        "    print(\"\\nMusic Recommendation Generated Successfully!!\")\n",
        "    with pd.option_context('display.max_rows', None,\n",
        "                           'display.max_columns', None,\n",
        "                           'display.width', 1000):\n",
        "\n",
        "        table_output = recommendations.to_string(index=False)\n",
        "\n",
        "        print(\"\\n=======================================================\")\n",
        "        print(f\"        FINAL RECOMMENDATION for {detected_emotion.upper()}\")\n",
        "        print(\"=======================================================\")\n",
        "        print(table_output)\n",
        "        print(\"=======================================================\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if not preprocess_music_data():\n",
        "        print(\"\\n Music Data Files are missed!!\")\n",
        "        exit()\n",
        "\n",
        "    print(\"\\nStarting Music Recommendation Engine\")\n",
        "    run_moodmate_demo(simulated_emotion='angry')\n",
        "    run_moodmate_demo(simulated_emotion='sad')\n",
        "\n",
        "    print(\"\\nMilestone 3 Completed Successfully!!!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xImQ2q8dgPJX",
        "outputId": "cf934c41-823a-4672-df96-6f8c2b72c845",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Music Data Preprocessing from Milestone1\n",
            "Music Data Preprocessing Completed successfully!!! File saved to: preprocessed_data/preprocessed_music_data.csv\n",
            "\n",
            "Starting Music Recommendation Engine\n",
            "\n",
            "Emotion Detected from Milestone 2 Output: Angry\n",
            "Model and Music Data files verified\n",
            "\n",
            "Recommendation for: ANGRY\n",
            "Searching for tracks matching tags: ['rock', 'metal', 'hardcore', 'grindcore', 'punk']\n",
            "\n",
            "Music Recommendation Generated Successfully!!\n",
            "\n",
            "=======================================================\n",
            "        FINAL RECOMMENDATION for ANGRY\n",
            "=======================================================\n",
            "                                                   name                     artists       genre  popularity\n",
            "                                             Pink Skies                  Zach Bryan        punk          85\n",
            "                                                Starboy       The Weeknd, Daft Punk        punk          84\n",
            "                                               ROCKSTAR                    Junior H        rock          78\n",
            "                                             West Coast                Lana Del Rey rock-n-roll          78\n",
            "              Praise The Lord (Da Shine) (feat. Skepta)          A$AP Rocky, Skepta        rock          78\n",
            "                                               ROCKSTAR                    Junior H      j-rock          78\n",
            "                             rockstar (feat. 21 Savage)      Post Malone, 21 Savage        rock          77\n",
            "Rocket Man (I Think It's Going To Be A Long, Long Time)                  Elton John        rock          77\n",
            "                                    I Love Rock 'N Roll Joan Jett & the Blackhearts rock-n-roll          76\n",
            "                                    I Love Rock 'N Roll Joan Jett & the Blackhearts        rock          76\n",
            "=======================================================\n",
            "\n",
            "Emotion Detected from Milestone 2 Output: Sad\n",
            "Model and Music Data files verified\n",
            "\n",
            "Recommendation for: SAD\n",
            "Searching for tracks matching tags: ['acoustic', 'blues', 'sad', 'lo-fi', 'gospel']\n",
            "\n",
            "Music Recommendation Generated Successfully!!\n",
            "\n",
            "=======================================================\n",
            "        FINAL RECOMMENDATION for SAD\n",
            "=======================================================\n",
            "                                   name                              artists    genre  popularity\n",
            "                                   SAD!                         XXXTENTACION      sad          75\n",
            "                            Alien Blues                             Vundabar    blues          74\n",
            "       Smooth Operator - Single Version                                 Sade      sad          74\n",
            "                Love Is Gone - Acoustic               SLANDER, Dylan Matthew acoustic          71\n",
            "The Sound of Silence - Acoustic Version                    Simon & Garfunkel acoustic          71\n",
            "                               Sad Girl                         Lana Del Rey      sad          71\n",
            "                          Like a Tattoo                                 Sade      sad          70\n",
            "                                 Sadqay Aashir Wajahat, NAYEL, Nehaal Naseem      sad          69\n",
            "                           Sad Loqueron                   Gabito Ballesteros      sad          68\n",
            "                           Kiss of Life                                 Sade      sad          68\n",
            "=======================================================\n",
            "\n",
            "Milestone 3 Completed Successfully!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import display, Javascript, HTML\n",
        "from google.colab.output import eval_js\n",
        "from google.colab import files\n",
        "import io\n",
        "from PIL import Image\n",
        "import base64\n",
        "\n",
        "OUTPUT_DIR = 'preprocessed_data'\n",
        "MODEL_SAVE_PATH = 'emotion_detection_cnn_model.keras'\n",
        "MUSIC_DATA_PATH = os.path.join(OUTPUT_DIR, 'preprocessed_music_data.csv')\n",
        "IMG_SIZE = 48\n",
        "EMOTION_LABELS = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
        "EMOTION_TO_MOOD_MAP = {\n",
        "    'happy': ['pop', 'dance', 'happy', 'party', 'upbeat', 'electronic'],\n",
        "    'sad': ['acoustic', 'blues', 'sad', 'lo-fi', 'gospel'],\n",
        "    'neutral': ['ambient', 'chill', 'study', 'sleep', 'jazz'],\n",
        "    'angry': ['rock', 'metal', 'hardcore', 'grindcore', 'punk'],\n",
        "    'fear': ['ambient', 'soundtracks', 'movies', 'drone'],\n",
        "    'surprise': ['dance', 'j-dance', 'club', 'techno'],\n",
        "    'disgust': ['industrial', 'metal-misc', 'goth']\n",
        "}\n",
        "try:\n",
        "    model = load_model(MODEL_SAVE_PATH)\n",
        "    print(f\"Model '{MODEL_SAVE_PATH}' loaded successfully!!\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not load the model... Details: {e}\")\n",
        "    exit()\n",
        "\n",
        "try:\n",
        "    music_df = pd.read_csv(MUSIC_DATA_PATH)\n",
        "    print(f\" Music Data loaded successfully !!!\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not load music data... Details: {e}\")\n",
        "    exit()\n",
        "\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.9):\n",
        "    js = Javascript('''\n",
        "        async function takePhoto(quality) {\n",
        "          const div = document.createElement('div');\n",
        "          const capture = document.createElement('button');\n",
        "          capture.textContent = 'Click to Capture Image';\n",
        "          capture.style.display = 'block';\n",
        "\n",
        "          const video = document.createElement('video');\n",
        "          video.style.display = 'block';\n",
        "\n",
        "          // Request camera access to detect Emotion\n",
        "          const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "          document.body.appendChild(div);\n",
        "          div.appendChild(video);\n",
        "          div.appendChild(capture);\n",
        "\n",
        "          video.srcObject = stream;\n",
        "          await video.play();\n",
        "\n",
        "          // Wait for button click\n",
        "          await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "          const canvas = document.createElement('canvas');\n",
        "          canvas.width = video.videoWidth;\n",
        "          canvas.height = video.videoHeight;\n",
        "          canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "          stream.getVideoTracks()[0].stop();\n",
        "          div.remove();\n",
        "\n",
        "          return new Promise(function(resolve) {\n",
        "              canvas.toBlob(function(blob) {\n",
        "                  const reader = new FileReader();\n",
        "                  reader.onloadend = function() {\n",
        "                      resolve(reader.result.split(',')[1]);\n",
        "                  };\n",
        "                  reader.readAsDataURL(blob);\n",
        "              }, 'image/jpeg', quality);\n",
        "          });\n",
        "        }\n",
        "        ''')\n",
        "    display(js)\n",
        "    data = eval_js('takePhoto({})'.format(quality))\n",
        "    binary = io.BytesIO(base64.b64decode(data))\n",
        "    pimg = Image.open(binary)\n",
        "    return cv2.cvtColor(np.array(pimg), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "def get_recommendation(emotion_label, music_df, top_n=10):\n",
        "    if emotion_label not in EMOTION_TO_MOOD_MAP: return pd.DataFrame()\n",
        "\n",
        "    required_tags = EMOTION_TO_MOOD_MAP[emotion_label.lower()]\n",
        "\n",
        "    def check_for_tags(tags_str):\n",
        "        if pd.isna(tags_str) or tags_str == '[]' or not tags_str: return False\n",
        "        tags_str_lower = str(tags_str).lower()\n",
        "        return any(tag in tags_str_lower for tag in required_tags)\n",
        "\n",
        "    filtered_df = music_df[music_df['Mood_Tags'].apply(check_for_tags)].copy()\n",
        "\n",
        "    if filtered_df.empty:\n",
        "        return music_df.sort_values(by='popularity', ascending=False).head(top_n)[['name', 'artists', 'genre', 'popularity']]\n",
        "\n",
        "    recommendations = filtered_df.sort_values(by='popularity', ascending=False).head(top_n)\n",
        "\n",
        "    return recommendations[['name', 'artists', 'genre', 'popularity']]\n",
        "\n",
        "def run_milestone4_demo():\n",
        "    frame = None\n",
        "    try:\n",
        "        frame = take_photo()\n",
        "    except Exception as e:\n",
        "        print(f\"\\nWebcam failed: {e}\")\n",
        "        pass\n",
        "\n",
        "    if frame is None:\n",
        "        print(\"\\nFILE UPLOAD\")\n",
        "        print(\"Upload Your Image to Detect Emotion\")\n",
        "\n",
        "        uploaded = files.upload()\n",
        "        if not uploaded:\n",
        "            print(\"No file uploaded. Cannot proceed.\")\n",
        "            return\n",
        "\n",
        "        file_name = list(uploaded.keys())[0]\n",
        "        frame = cv2.imdecode(np.frombuffer(uploaded[file_name], np.uint8), cv2.IMREAD_COLOR)\n",
        "\n",
        "        if frame is None:\n",
        "            print(\"Can't read image file... Ensure it is a valid JPG/PNG.\")\n",
        "            return\n",
        "\n",
        "        print(f\"Image '{file_name}' loaded successfully \")\n",
        "\n",
        "\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect Face\n",
        "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "\n",
        "    if len(faces) == 0:\n",
        "        print(\"\\nFace not detected in the image. Please use a clear image to Detect a image....\")\n",
        "        cv2_imshow(frame)\n",
        "        return\n",
        "\n",
        "    (x, y, w, h) = sorted(faces, key=lambda f: f[2] * f[3], reverse=True)[0]\n",
        "\n",
        "\n",
        "    roi_gray = gray[y:y + h, x:x + w]\n",
        "    cropped_img = cv2.resize(roi_gray, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "\n",
        "    reshaped_img = np.expand_dims(np.expand_dims(cropped_img / 255.0, -1), 0)\n",
        "\n",
        "    prediction = model.predict(reshaped_img, verbose=0)[0]\n",
        "    emotion_index = np.argmax(prediction)\n",
        "    detected_emotion = EMOTION_LABELS[emotion_index]\n",
        "    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "    cv2.putText(frame, detected_emotion.upper(), (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "    print(f\"\\nEmotion Detected by CNN Model: {detected_emotion.upper()}\")\n",
        "    cv2_imshow(frame)\n",
        "    recommendations = get_recommendation(detected_emotion, music_df)\n",
        "    with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 1000):\n",
        "        table_output = recommendations.to_string(index=False)\n",
        "\n",
        "        print(\"\\n=========================================================================\")\n",
        "        print(f\"        MoodMate Playlist for Detected Emotion{detected_emotion.upper()} \")\n",
        "        print(\"=========================================================================\")\n",
        "        print(table_output)\n",
        "        print(\"=========================================================================\")\n",
        "\n",
        "    print(\"\\n Milestone 4 Completed Successfully !!!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_milestone4_demo()"
      ],
      "metadata": {
        "id": "AoWvNAgXGex5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}